# Project: Data Warehouse for Sparkify


## Purpose
The purpose of this project is to build an ETL pipeline that extracts Sparkify's data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. Then Sparkify analytics team will be able to use these new data to continue finding insights in what songs their users are listening to.


## Database Schema and Design
As of today, Sparkify collects all their data and store them in [Amazon S3](https://s3.console.aws.amazon.com/s3/buckets/udacity-dend). There are 2 sets of data: [song data](https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/song_data) and [log data](https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/log_data).  The song data contains metadata about a song and artist of a song, e.g. artist name, song title, artist location, etc. Its files are partitioned by the first three letters of each song's track ID and it's stored in JSON format. The song data JSON file looks like the following:

```
    {
        "artist_id":"ARJNIUY12298900C91",
        "artist_latitude":null,
        "artist_location":"",
        "artist_longitude":null,
        "artist_name":"Adelitas Way",
        "duration":213.9424,
        "num_songs":1,
        "song_id":"SOBLFFE12AF72AA5BA",
        "title":"Scream",
        "year":2009
    }
```

Meanwhile, log data consists activity log files generated by music streaming app based on the songs in the dataset, e.g. user first name, user last name, artist name, song title, etc. Its files are partitioned by year and month and stored in JSON format. The log data JSON file looks like the following:

```
    {
        "artist":null,
        "auth":"Logged In",
        "firstName":"Walter",
        "gender":"M",
        "itemInSession":0,
        "lastName":"Frye",
        "length":null,
        "level":"free",
        "location":"San Francisco-Oakland-Hayward, CA",
        "method":"GET",
        "page":"Home",
        "registration":1540919166796.0,
        "sessionId":38,
        "song":null,
        "status":200,
        "ts":1541105830796,
        "userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"",
        "userId":"39"
    }
    {
        ...
    }
```

To optimize the query on song play analysis, we will ELT (Extract, Load & Transform) these data to fact and dimension tables through AWS EMR cluster and store them back in AWS S3 bucket. This will also allow us to do on-the-fly at the time of analysis of the data. Below is how the fact and dimension tables will look like:

***Fact Table***

songplays - records in log data associated with song plays i.e. records with page NextSong.<br>
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

***Dimension Tables***
1. users - users in the app.<br>
   user_id, first_name, last_name, gender, level
2. songs - songs in music database.<br>
   song_id, title, artist_id, year, duration
3. artists - artists in music database.<br>
   artist_id, name, location, lattitude, longitude 
4. time - timestamps of records in songplays broken down into specific units.<br>
   start_time, hour, day, week, month, year, weekday


## Instructions
Before creating the cluster and all the tables, we will need to enter AWS secret key & password and input & output data in a config file (dl.cfg). Then we will run "etl.py" to extract data from AWS S3, load, and transform them to fact and dimension tables. Then store them back to AWS S3


## Author
Budi Sulayman


## License
This project is licensed under [MIT](https://choosealicense.com/licenses/mit/)